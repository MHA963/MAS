{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step-by-Step Solution for MAS Assignments using ir-sim\n",
        "\n",
        "This notebook provides implementations for the three assignments. We use ir-sim for simulations. Assume ir-sim is installed (`pip install ir-sim`). For RL, we wrap ir-sim in Gymnasium and use Stable-Baselines3 (`pip install stable-baselines3 gymnasium`).\n",
        "\n",
        "Focus: Precise code, metrics, evaluations. Run cells sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports (common for all)\n",
        "import irsim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gymnasium import Env, spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "import yaml\n",
        "\n",
        "\n",
        "# For metrics\n",
        "def compute_circle_metric(positions, center, radius):\n",
        "    distances = np.linalg.norm(np.array(positions) - center, axis=1)\n",
        "    return np.mean(np.abs(distances - radius))\n",
        "\n",
        "def collision_rate(env):\n",
        "    return env.world.check_all_collisions()  # ir-sim has collision check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 1: Basic - RL for Circle Following\n",
        "\n",
        "Goal: Train single agent to follow circle around (5,5) r=3. Then scale to 5-10 agents with shared policy.\n",
        "\n",
        "Steps:\n",
        "1. Define Gym env for circle.\n",
        "2. Train PPO.\n",
        "3. Simulate MAS, compute metrics: mean deviation from circle, no collisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import irsim\n",
        "\n",
        "env = irsim.make(\"circle.yaml\")\n",
        "env.load_behavior(\"custom_behavior_methods\")  # load the custom behavior module\n",
        "\n",
        "STEPS = 1000\n",
        "for _ in range(STEPS):\n",
        "    env.step()\n",
        "    env.render(0.01)  # small pause for visualization\n",
        "\n",
        "    if env.done():\n",
        "        break\n",
        "\n",
        "# Optional: compute error from desired circle\n",
        "state = env.get_robot_state()[0]\n",
        "pos = state[:2]\n",
        "error = abs(np.linalg.norm(pos - [5,5]) - 2.0)\n",
        "print(\"Distance from circle:\", error)\n",
        "\n",
        "env.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 1: Advanced - Subsumption Architecture\n",
        "\n",
        "Layers: 1. Obstacle avoidance (RVO). 2. Circle following (custom controller).\n",
        "\n",
        "Steps:\n",
        "1. Implement layered control.\n",
        "2. Evaluate single, then scale to 5,10,20. Metrics: agent-level (deviation, speed), group (coverage, collisions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Subsumption Controller\n",
        "class SubsumptionController:\n",
        "    def __init__(self, center, radius):\n",
        "        self.center = np.array(center)\n",
        "        self.radius = radius\n",
        "    \n",
        "    def layer1_avoid(self, percepts):\n",
        "        # Use RVO or simple: if obstacle <1m, turn\n",
        "        if any(d < 1 for d in percepts):\n",
        "            return np.array([0, np.pi/4])  # Suppress higher, activate avoid\n",
        "        return None\n",
        "    \n",
        "    def layer2_circle(self, pos):\n",
        "        # Tangential velocity for circle\n",
        "        to_center = self.center - pos\n",
        "        tangent = np.array([-to_center[1], to_center[0]]) / np.linalg.norm(to_center)\n",
        "        vel = 0.5 * tangent[:2]  # Linear vel tangential\n",
        "        ang = 0  # Straight\n",
        "        return np.append(vel, ang)\n",
        "    \n",
        "    def decide(self, pos, percepts):\n",
        "        avoid = self.layer1_avoid(percepts)\n",
        "        if avoid is not None:\n",
        "            return avoid\n",
        "        return self.layer2_circle(pos)\n",
        "\n",
        "# Sim single\n",
        "env = irsim.make('circle_obs.yaml')  # With obstacles\n",
        "controller = SubsumptionController([5,5], 3)\n",
        "for i in range(500):\n",
        "    pos = env.get_state(0)[:2]\n",
        "    percepts = env.get_lidar_scan(0)\n",
        "    action = controller.decide(pos, percepts)\n",
        "    env.step(action)\n",
        "    env.render(0.05)\n",
        "env.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Scaling Evaluation\n",
        "def evaluate_subsumption(n_robots):\n",
        "    # Similar to MAS, but use controller per agent\n",
        "    # Track per agent deviation, avg speed, group collisions\n",
        "    # ... implement loop, return dict of metrics\n",
        "    pass  # Placeholder - expand as above\n",
        "\n",
        "for n in [1,5,10,20]:\n",
        "    metrics = evaluate_subsumption(n)\n",
        "    print(f'N={n}: {metrics}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 2: Basic - Nature-Inspired Leader-Follower (Boids)\n",
        "\n",
        "5 agents, fixed leader. Followers use separation, alignment, cohesion via FOV (no comm).\n",
        "\n",
        "Steps:\n",
        "1. Implement boids rules.\n",
        "2. Sim, metrics: formation distance variance, distance to leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Boids Controller\n",
        "class BoidsFollower:\n",
        "    def __init__(self, leader_pos, fov_range=2.0):\n",
        "        self.leader = leader_pos\n",
        "        self.range = fov_range\n",
        "        self.separation_w = 1.5\n",
        "        self.alignment_w = 1.0\n",
        "        self.cohesion_w = 1.0\n",
        "    \n",
        "    def perceive_neighbors(self, env, agent_id):\n",
        "        # Use FOV to get nearby agents\n",
        "        neighbors = []\n",
        "        for i in range(env.robot_num):\n",
        "            if i != agent_id and np.linalg.norm(env.get_state(i)[:2] - env.get_state(agent_id)[:2]) < self.range:\n",
        "                neighbors.append(env.get_state(i))\n",
        "        return np.array(neighbors)\n",
        "    \n",
        "    def compute_velocity(self, pos, vel, neighbors):\n",
        "        if len(neighbors) == 0:\n",
        "            # Follow leader\n",
        "            dir_to_leader = self.leader - pos\n",
        "            return 0.5 * dir_to_leader / np.linalg.norm(dir_to_leader)\n",
        "        \n",
        "        # Separation\n",
        "        sep = np.mean([pos - n[:2] for n in neighbors], axis=0)\n",
        "        \n",
        "        # Alignment\n",
        "        ali = np.mean([n[2:4] for n in neighbors], axis=0)  # Assume vel in state\n",
        "        \n",
        "        # Cohesion\n",
        "        coh = np.mean([n[:2] for n in neighbors], axis=0) - pos\n",
        "        \n",
        "        new_vel = (self.separation_w * sep + self.alignment_w * ali + self.cohesion_w * coh)\n",
        "        return 0.5 * new_vel / np.linalg.norm(new_vel)\n",
        "\n",
        "# Sim\n",
        "env = irsim.make('flock.yaml')  # 5 robots, leader at [0,0]\n",
        "leader_pos = [0,0]\n",
        "followers = [BoidsFollower(leader_pos) for _ in range(4)]  # Agent 0 leader\n",
        "for t in range(1000):\n",
        "    for i in range(1,5):\n",
        "        pos = env.get_state(i)[:2]\n",
        "        vel = env.get_state(i)[2:4]\n",
        "        neighbors = followers[i-1].perceive_neighbors(env, i)\n",
        "        action = followers[i-1].compute_velocity(pos, vel, neighbors)\n",
        "        env.step(action)\n",
        "    env.render(0.05)\n",
        "    if env.done(): break\n",
        "env.end()\n",
        "\n",
        "# Metrics: variance of inter-distances\n",
        "# Implement tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 2: Advanced - Leader Election + Coordination\n",
        "\n",
        "Bully election, then assign positions in line, use virtual comm (ir-sim message).\n",
        "\n",
        "Steps:\n",
        "1. Implement bully election.\n",
        "2. Post-election, coordinate formation.\n",
        "3. Scale, metrics: election time, formation stability.\n",
        "4. Reflection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Bully Election (simplified, IDs 1-10, highest wins)\n",
        "class BullyElection:\n",
        "    def __init__(self, agent_id):\n",
        "        self.id = agent_id\n",
        "        self.state = 'alive'  # alive, candidate, defeated\n",
        "        self.messages = []  # Simulate comm\n",
        "    \n",
        "    def election_phase(self, all_agents):\n",
        "        # Broadcast election if alive\n",
        "        if self.state == 'alive':\n",
        "            self.state = 'candidate'\n",
        "            for agent in all_agents:\n",
        "                if agent.id > self.id and agent.state != 'defeated':\n",
        "                    agent.messages.append(('election', self.id))\n",
        "        \n",
        "        # Handle messages\n",
        "        for msg_type, sender_id in self.messages:\n",
        "            if msg_type == 'election' and sender_id > self.id:\n",
        "                self.state = 'defeated'\n",
        "            elif msg_type == 'victory':\n",
        "                self.state = 'follower'\n",
        "        self.messages = []\n",
        "        \n",
        "        if all(a.state != 'candidate' for a in all_agents if a.id != self.id):\n",
        "            if self.state == 'candidate':\n",
        "                for a in all_agents:\n",
        "                    a.messages.append(('victory', self.id))\n",
        "                self.state = 'leader'\n",
        "\n",
        "# Sim election\n",
        "agents = [BullyElection(i) for i in range(1,6)]\n",
        "for round in range(10):  # Max rounds\n",
        "    for agent in agents:\n",
        "        agent.election_phase(agents)\n",
        "    if any(a.state == 'leader' for a in agents): break\n",
        "leader_id = next(a.id for a in agents if a.state == 'leader')\n",
        "print(f'Leader: {leader_id}')\n",
        "\n",
        "# Step 2: Formation (line behind leader)\n",
        "# Assign positions based on ID order, use boids to maintain\n",
        "# ... extend boids with target positions\n",
        "\n",
        "# Step 3: Scaling - repeat for n=10,20, measure rounds for election\n",
        "\n",
        "# Reflection: Bully pros: simple, decentralized; cons: high msg overhead for large n. Nature-inspired: low comm, emergent; cons: no guarantee on leader quality. Superior: Bully for critical systems needing quick consensus; boids for robust swarms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 3: B - MARL for Level-Based Foraging\n",
        "\n",
        "Use ir-sim grid map with levels (1-3 items). Agents collect, upgrade. Use simple QMIX-like (torch) or independent PPO.\n",
        "\n",
        "Steps:\n",
        "1. Setup foraging env.\n",
        "2. Implement MARL (independent learners).\n",
        "3. Scale 5-20 agents, metrics: total collected, time to max level, collisions.\n",
        "4. Reflection: Scalability issues with non-coop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Foraging Env (grid with items)\n",
        "class ForagingEnv(Env):\n",
        "    # Similar to CircleEnv, but multi-agent, state includes item levels\n",
        "    # Items on map, agents carry level, collect if match/lower\n",
        "    pass  # Define: obs per agent (pos, inventory, local map), action move/collect\n",
        "\n",
        "# Step 2: MARL Training (Independent PPO)\n",
        "n_agents = 5\n",
        "models = [PPO('MlpPolicy', ForagingEnv()) for _ in range(n_agents)]\n",
        "for model in models:\n",
        "    model.learn(10000)\n",
        "\n",
        "# Step 3: Sim & Scale\n",
        "def eval_foraging(n):\n",
        "    # Load models, sim, track collected items sum\n",
        "    pass\n",
        "\n",
        "for n in [5,10,20]:\n",
        "    score = eval_foraging(n)\n",
        "    print(f'N={n}: Total collected {score}')\n",
        "\n",
        "# Reflection: Independent MARL scales poorly due to non-stationarity; centralized critic (QMIX) better for coop. Performance drops with density."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MAS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
