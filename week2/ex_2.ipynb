{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step-by-Step Solution for MAS Assignments using ir-sim\n",
        "\n",
        "This notebook provides implementations for the three assignments. We use ir-sim for simulations. Assume ir-sim is installed (`pip install ir-sim`). For RL, we wrap ir-sim in Gymnasium and use Stable-Baselines3 (`pip install stable-baselines3 gymnasium`).\n",
        "\n",
        "Focus: Precise code, metrics, evaluations. Run cells sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports (common for all)\n",
        "import irsim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gymnasium import Env, spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "import yaml\n",
        "\n",
        "\n",
        "# For metrics\n",
        "def compute_circle_metric(positions, center, radius):\n",
        "    distances = np.linalg.norm(np.array(positions) - center, axis=1)\n",
        "    return np.mean(np.abs(distances - radius))\n",
        "\n",
        "def collision_rate(env):\n",
        "    return env.world.check_all_collisions()  # ir-sim has collision check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 1: Basic - RL for Circle Following\n",
        "\n",
        "Goal: Train single agent to follow circle around (5,5) r=3. Then scale to 5-10 agents with shared policy.\n",
        "\n",
        "Steps:\n",
        "1. Define Gym env for circle.\n",
        "2. Train PPO.\n",
        "3. Simulate MAS, compute metrics: mean deviation from circle, no collisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-22 22:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSimulation environment 'circle' has been initialized and started.\u001b[0m\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m state = env.get_robot_state()[\u001b[32m0\u001b[39m]\n\u001b[32m     16\u001b[39m pos = state[:\u001b[32m2\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m error = \u001b[38;5;28mabs\u001b[39m(\u001b[43mnp\u001b[49m.linalg.norm(pos - [\u001b[32m5\u001b[39m,\u001b[32m5\u001b[39m]) - \u001b[32m2.0\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDistance from circle:\u001b[39m\u001b[33m\"\u001b[39m, error)\n\u001b[32m     20\u001b[39m env.end()\n",
            "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "import irsim\n",
        "\n",
        "env = irsim.make(\"circle.yaml\")\n",
        "env.load_behavior(\"custom_behavior_methods\")  # load the custom behavior module\n",
        "\n",
        "STEPS = 1000\n",
        "for _ in range(STEPS):\n",
        "    env.step()\n",
        "    env.render(0.01)  # small pause for visualization\n",
        "\n",
        "    if env.done():\n",
        "        break\n",
        "\n",
        "# Optional: compute error from desired circle\n",
        "state = env.get_robot_state()[0]\n",
        "pos = state[:2]\n",
        "error = abs(np.linalg.norm(pos - [5,5]) - 2.0)\n",
        "print(\"Distance from circle:\", error)\n",
        "\n",
        "env.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "29007b4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-22 22:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSimulation environment 'circle' has been initialized and started.\u001b[0m\n",
            "\u001b[32m2025-09-22 22:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mFigure will be closed within 3.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-09-22 22:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mThe simulated environment has ended. Total simulation time: 0.1 seconds.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import irsim\n",
        "\n",
        "env = irsim.make('circle.yaml') # initialize the environment with the configuration file\n",
        "\n",
        "for i in range(300): # run the simulation for 300 steps\n",
        "\n",
        "    env.step()  # update the environment\n",
        "    env.render() # render the environment\n",
        "\n",
        "    if env.done(): break # check if the simulation is done\n",
        "        \n",
        "env.end() # close the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-22 21:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mSimulation environment 'circle' has been initialized and started.\u001b[0m\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'EnvBase' object has no attribute 'get_state'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 2: Train PPO\u001b[39;00m\n\u001b[32m      2\u001b[39m env = CircleEnv()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mcheck_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m model = PPO(\u001b[33m'\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m'\u001b[39m, env, verbose=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m model.learn(total_timesteps=\u001b[32m10000\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hadev\\anaconda3\\envs\\MAS\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:442\u001b[39m, in \u001b[36mcheck_env\u001b[39m\u001b[34m(env, warn, skip_render_check)\u001b[39m\n\u001b[32m    439\u001b[39m action_space = env.action_space\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe reset() method must accept a `seed` parameter\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mCircleEnv.reset\u001b[39m\u001b[34m(self, seed)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mself\u001b[39m.env.reset()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state\u001b[49m(\u001b[32m0\u001b[39m)[:\u001b[32m4\u001b[39m]  \u001b[38;5;66;03m# position and velocity\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, {}\n",
            "\u001b[31mAttributeError\u001b[39m: 'EnvBase' object has no attribute 'get_state'"
          ]
        }
      ],
      "source": [
        "# Step 2: Train PPO\n",
        "env = CircleEnv()\n",
        "check_env(env)\n",
        "model = PPO('MlpPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=10000)\n",
        "env.close()\n",
        "model.save('circle_ppo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: MAS Simulation (5 robots)\n",
        "def mas_circle(n_robots=5):\n",
        "    # Modify YAML for multi\n",
        "    multi_yaml = yaml_config.replace('robot:', f'robots:\\n' + '  - ' * n_robots)\n",
        "    # Positions staggered\n",
        "    states = [[i*0.5+1,1,0] for i in range(n_robots)]\n",
        "    # ... (expand YAML with states)\n",
        "    \n",
        "    env = irsim.make('multi_circle.yaml')  # Assume updated YAML\n",
        "    model = PPO.load('circle_ppo')\n",
        "    positions = []\n",
        "    for t in range(1000):\n",
        "        for i in range(n_robots):\n",
        "            obs = env.get_state(i)[:4]\n",
        "            action, _ = model.predict(obs)\n",
        "            env.step(i, action)\n",
        "        positions.append([env.get_state(i)[:2] for i in range(n_robots)])\n",
        "        env.render(0.05)\n",
        "        if env.done(): break\n",
        "    env.end()\n",
        "    positions = np.array(positions)\n",
        "    metric = compute_circle_metric(positions.mean(axis=1), [5,5], 3)\n",
        "    coll = collision_rate(env)  # Track during sim\n",
        "    print(f'Mean deviation: {metric}, Collisions: {coll}')\n",
        "\n",
        "mas_circle(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 1: Advanced - Subsumption Architecture\n",
        "\n",
        "Layers: 1. Obstacle avoidance (RVO). 2. Circle following (custom controller).\n",
        "\n",
        "Steps:\n",
        "1. Implement layered control.\n",
        "2. Evaluate single, then scale to 5,10,20. Metrics: agent-level (deviation, speed), group (coverage, collisions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Subsumption Controller\n",
        "class SubsumptionController:\n",
        "    def __init__(self, center, radius):\n",
        "        self.center = np.array(center)\n",
        "        self.radius = radius\n",
        "    \n",
        "    def layer1_avoid(self, percepts):\n",
        "        # Use RVO or simple: if obstacle <1m, turn\n",
        "        if any(d < 1 for d in percepts):\n",
        "            return np.array([0, np.pi/4])  # Suppress higher, activate avoid\n",
        "        return None\n",
        "    \n",
        "    def layer2_circle(self, pos):\n",
        "        # Tangential velocity for circle\n",
        "        to_center = self.center - pos\n",
        "        tangent = np.array([-to_center[1], to_center[0]]) / np.linalg.norm(to_center)\n",
        "        vel = 0.5 * tangent[:2]  # Linear vel tangential\n",
        "        ang = 0  # Straight\n",
        "        return np.append(vel, ang)\n",
        "    \n",
        "    def decide(self, pos, percepts):\n",
        "        avoid = self.layer1_avoid(percepts)\n",
        "        if avoid is not None:\n",
        "            return avoid\n",
        "        return self.layer2_circle(pos)\n",
        "\n",
        "# Sim single\n",
        "env = irsim.make('circle_obs.yaml')  # With obstacles\n",
        "controller = SubsumptionController([5,5], 3)\n",
        "for i in range(500):\n",
        "    pos = env.get_state(0)[:2]\n",
        "    percepts = env.get_lidar_scan(0)\n",
        "    action = controller.decide(pos, percepts)\n",
        "    env.step(action)\n",
        "    env.render(0.05)\n",
        "env.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Scaling Evaluation\n",
        "def evaluate_subsumption(n_robots):\n",
        "    # Similar to MAS, but use controller per agent\n",
        "    # Track per agent deviation, avg speed, group collisions\n",
        "    # ... implement loop, return dict of metrics\n",
        "    pass  # Placeholder - expand as above\n",
        "\n",
        "for n in [1,5,10,20]:\n",
        "    metrics = evaluate_subsumption(n)\n",
        "    print(f'N={n}: {metrics}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 2: Basic - Nature-Inspired Leader-Follower (Boids)\n",
        "\n",
        "5 agents, fixed leader. Followers use separation, alignment, cohesion via FOV (no comm).\n",
        "\n",
        "Steps:\n",
        "1. Implement boids rules.\n",
        "2. Sim, metrics: formation distance variance, distance to leader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Boids Controller\n",
        "class BoidsFollower:\n",
        "    def __init__(self, leader_pos, fov_range=2.0):\n",
        "        self.leader = leader_pos\n",
        "        self.range = fov_range\n",
        "        self.separation_w = 1.5\n",
        "        self.alignment_w = 1.0\n",
        "        self.cohesion_w = 1.0\n",
        "    \n",
        "    def perceive_neighbors(self, env, agent_id):\n",
        "        # Use FOV to get nearby agents\n",
        "        neighbors = []\n",
        "        for i in range(env.robot_num):\n",
        "            if i != agent_id and np.linalg.norm(env.get_state(i)[:2] - env.get_state(agent_id)[:2]) < self.range:\n",
        "                neighbors.append(env.get_state(i))\n",
        "        return np.array(neighbors)\n",
        "    \n",
        "    def compute_velocity(self, pos, vel, neighbors):\n",
        "        if len(neighbors) == 0:\n",
        "            # Follow leader\n",
        "            dir_to_leader = self.leader - pos\n",
        "            return 0.5 * dir_to_leader / np.linalg.norm(dir_to_leader)\n",
        "        \n",
        "        # Separation\n",
        "        sep = np.mean([pos - n[:2] for n in neighbors], axis=0)\n",
        "        \n",
        "        # Alignment\n",
        "        ali = np.mean([n[2:4] for n in neighbors], axis=0)  # Assume vel in state\n",
        "        \n",
        "        # Cohesion\n",
        "        coh = np.mean([n[:2] for n in neighbors], axis=0) - pos\n",
        "        \n",
        "        new_vel = (self.separation_w * sep + self.alignment_w * ali + self.cohesion_w * coh)\n",
        "        return 0.5 * new_vel / np.linalg.norm(new_vel)\n",
        "\n",
        "# Sim\n",
        "env = irsim.make('flock.yaml')  # 5 robots, leader at [0,0]\n",
        "leader_pos = [0,0]\n",
        "followers = [BoidsFollower(leader_pos) for _ in range(4)]  # Agent 0 leader\n",
        "for t in range(1000):\n",
        "    for i in range(1,5):\n",
        "        pos = env.get_state(i)[:2]\n",
        "        vel = env.get_state(i)[2:4]\n",
        "        neighbors = followers[i-1].perceive_neighbors(env, i)\n",
        "        action = followers[i-1].compute_velocity(pos, vel, neighbors)\n",
        "        env.step(action)\n",
        "    env.render(0.05)\n",
        "    if env.done(): break\n",
        "env.end()\n",
        "\n",
        "# Metrics: variance of inter-distances\n",
        "# Implement tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 2: Advanced - Leader Election + Coordination\n",
        "\n",
        "Bully election, then assign positions in line, use virtual comm (ir-sim message).\n",
        "\n",
        "Steps:\n",
        "1. Implement bully election.\n",
        "2. Post-election, coordinate formation.\n",
        "3. Scale, metrics: election time, formation stability.\n",
        "4. Reflection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Bully Election (simplified, IDs 1-10, highest wins)\n",
        "class BullyElection:\n",
        "    def __init__(self, agent_id):\n",
        "        self.id = agent_id\n",
        "        self.state = 'alive'  # alive, candidate, defeated\n",
        "        self.messages = []  # Simulate comm\n",
        "    \n",
        "    def election_phase(self, all_agents):\n",
        "        # Broadcast election if alive\n",
        "        if self.state == 'alive':\n",
        "            self.state = 'candidate'\n",
        "            for agent in all_agents:\n",
        "                if agent.id > self.id and agent.state != 'defeated':\n",
        "                    agent.messages.append(('election', self.id))\n",
        "        \n",
        "        # Handle messages\n",
        "        for msg_type, sender_id in self.messages:\n",
        "            if msg_type == 'election' and sender_id > self.id:\n",
        "                self.state = 'defeated'\n",
        "            elif msg_type == 'victory':\n",
        "                self.state = 'follower'\n",
        "        self.messages = []\n",
        "        \n",
        "        if all(a.state != 'candidate' for a in all_agents if a.id != self.id):\n",
        "            if self.state == 'candidate':\n",
        "                for a in all_agents:\n",
        "                    a.messages.append(('victory', self.id))\n",
        "                self.state = 'leader'\n",
        "\n",
        "# Sim election\n",
        "agents = [BullyElection(i) for i in range(1,6)]\n",
        "for round in range(10):  # Max rounds\n",
        "    for agent in agents:\n",
        "        agent.election_phase(agents)\n",
        "    if any(a.state == 'leader' for a in agents): break\n",
        "leader_id = next(a.id for a in agents if a.state == 'leader')\n",
        "print(f'Leader: {leader_id}')\n",
        "\n",
        "# Step 2: Formation (line behind leader)\n",
        "# Assign positions based on ID order, use boids to maintain\n",
        "# ... extend boids with target positions\n",
        "\n",
        "# Step 3: Scaling - repeat for n=10,20, measure rounds for election\n",
        "\n",
        "# Reflection: Bully pros: simple, decentralized; cons: high msg overhead for large n. Nature-inspired: low comm, emergent; cons: no guarantee on leader quality. Superior: Bully for critical systems needing quick consensus; boids for robust swarms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 3: B - MARL for Level-Based Foraging\n",
        "\n",
        "Use ir-sim grid map with levels (1-3 items). Agents collect, upgrade. Use simple QMIX-like (torch) or independent PPO.\n",
        "\n",
        "Steps:\n",
        "1. Setup foraging env.\n",
        "2. Implement MARL (independent learners).\n",
        "3. Scale 5-20 agents, metrics: total collected, time to max level, collisions.\n",
        "4. Reflection: Scalability issues with non-coop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Foraging Env (grid with items)\n",
        "class ForagingEnv(Env):\n",
        "    # Similar to CircleEnv, but multi-agent, state includes item levels\n",
        "    # Items on map, agents carry level, collect if match/lower\n",
        "    pass  # Define: obs per agent (pos, inventory, local map), action move/collect\n",
        "\n",
        "# Step 2: MARL Training (Independent PPO)\n",
        "n_agents = 5\n",
        "models = [PPO('MlpPolicy', ForagingEnv()) for _ in range(n_agents)]\n",
        "for model in models:\n",
        "    model.learn(10000)\n",
        "\n",
        "# Step 3: Sim & Scale\n",
        "def eval_foraging(n):\n",
        "    # Load models, sim, track collected items sum\n",
        "    pass\n",
        "\n",
        "for n in [5,10,20]:\n",
        "    score = eval_foraging(n)\n",
        "    print(f'N={n}: Total collected {score}')\n",
        "\n",
        "# Reflection: Independent MARL scales poorly due to non-stationarity; centralized critic (QMIX) better for coop. Performance drops with density."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MAS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
